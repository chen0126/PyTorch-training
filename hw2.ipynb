{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOMV6M7Kt+4blkVaeKGGhE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chen0126/pytorch-training/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnjEV9QSCQG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "7911d6aa-6342-4b0b-e9d1-44cb75c9af3c"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import numpy\n",
        "import json\n",
        "\n",
        "with open(\"/hw2/30702/scale.json\",\"r\") as f:\n",
        "    scales = json.load(f)\n",
        "\n",
        "conv1_weights = np.loadtxt((\"/hw2/30702/conv1.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "conv1_weights = conv1_weights.reshape(6,3,5,5)\n",
        "\n",
        "conv2_weights = np.loadtxt((\"/hw2/30702/conv2.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "conv2_weights = conv2_weights.reshape(16,6,5,5)\n",
        "\n",
        "fc1_weights = np.loadtxt((\"/hw2/30702/fc1.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "fc1_weights = fc1_weights.reshape(120,400)\n",
        "\n",
        "fc2_weights = np.loadtxt((\"/hw2/30702/fc2.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "fc2_weights = fc2_weights.reshape(84,120)\n",
        "\n",
        "fc3_weights = np.loadtxt((\"/hw2/30702/fc3.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "fc3_weights = fc3_weights.reshape(10,84)\n",
        "\n",
        "fc3_bias = np.loadtxt((\"/hw2/30702/fc3.bias.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "fc3_bias = fc3_bias.reshape(10)\n",
        "\n",
        "input_data = np.loadtxt((\"/hw2/30702/input.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "input_data = input_data.reshape(1,3,32,32)\n",
        "print(\"input_data.shape[1]: \",input_data.shape[1])\n",
        "print(\"weights.shape[0]: \",fc1_weights.shape[0])\n",
        "\n",
        "# with open(\"/content/scale.json\",\"r\") as f:\n",
        "#     scales = json.load(f)\n",
        "\n",
        "# conv1_weights = np.loadtxt((\"/content/weights/conv1.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "# conv1_weights = conv1_weights.reshape(6,3,5,5)\n",
        "\n",
        "# conv2_weights = np.loadtxt((\"/content/weights/conv2.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "# conv2_weights = conv2_weights.reshape(16,6,5,5)\n",
        "\n",
        "# fc1_weights = np.loadtxt((\"/content/weights/fc1.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "# fc1_weights = fc1_weights.reshape(120,400)\n",
        "\n",
        "# fc2_weights = np.loadtxt((\"/content/weights/fc2.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "# fc2_weights = fc2_weights.reshape(84,120)\n",
        "\n",
        "# fc3_weights = np.loadtxt((\"/content/weights/fc3.weight.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "# fc3_weights = fc3_weights.reshape(10,84)\n",
        "\n",
        "# fc3_bias = np.loadtxt((\"/content/weights/fc3.bias.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "# fc3_bias = fc3_bias.reshape(10)\n",
        "\n",
        "# input_data = np.loadtxt((\"/content/activations/input.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "# input_data = input_data.reshape(1,3,32,32)\n",
        "\n",
        "# #output_data = np.loadtxt((\"/hw2/1300/output.csv\") , delimiter=\",\" , skiprows = 0)\n",
        "# #output_data = output_data.reshape(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a1959c78b8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/hw2/30702/scale.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mscales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/hw2/30702/scale.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xe7E0JIsfoPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f250e8d-5a13-4b97-8ff1-09ac55208b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxIcp_LZkZW6"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpoxCArLCX16"
      },
      "source": [
        "import numba as nb\n",
        "\n",
        "@nb.jit()\n",
        "def convolution(N,M,P,Q,R,S,C,input_data,weights):\n",
        "    output_data = np.zeros((N,M,P,Q))\n",
        "    for n in range(N): \n",
        "      for m in range(M):\n",
        "        for p in range(P):\n",
        "          for q in range(Q):\n",
        "            output_data[n][m][p][q] = 0\n",
        "            for r in range(R):\n",
        "              for s in range(S):\n",
        "                for c in range(C):\n",
        "                  h = p + r\n",
        "                  w = q + s\n",
        "                  output_data[n][m][p][q] += input_data[n][c][h][w] * weights[m][c][r][s]\n",
        "    return output_data\n",
        "\n",
        "@nb.jit()\n",
        "def pooling(N,M,P,Q,input_data):\n",
        "    output_data = np.zeros((N,M,P,Q))\n",
        "    for n in range(N): \n",
        "      for m in range(M):\n",
        "        for p in range(P):\n",
        "          for q in range(Q):\n",
        "            output_data[n][m][p][q] = max(input_data[n][m][2*p][2*q],input_data[n][m][2*p+1][2*q],input_data[n][m][2*p][2*q+1],input_data[n][m][2*p+1][2*q+1])\n",
        "    return output_data\n",
        "\n",
        "@nb.jit()\n",
        "def fully_connected(N,input_data,weights):\n",
        "    X = input_data.shape[1]\n",
        "    Y = weights.shape[0]\n",
        "    output_data = np.zeros((N,Y))\n",
        "    for n in range(N):\n",
        "      for y in range(Y):\n",
        "        for x in range(X):\n",
        "          output_data[n][y] += input_data[n][x] * weights[y][x] \n",
        "    return output_data\n",
        "\n",
        "@nb.jit()\n",
        "def relu(OA):\n",
        "    OA = np.maximum(OA,0)\n",
        "    return OA\n",
        "\n",
        "@nb.jit()\n",
        "def forward_C(N,x):\n",
        "    # plt.figure(figsize=(20,10))\n",
        "\n",
        "    x = x * scales['input_scale']\n",
        "    x = np.clip(x.round(),-128,127)\n",
        "    x = convolution(N,6,28,28,5,5,3,x,conv1_weights)\n",
        "    x = np.clip(x,-2**17,2**17-1)\n",
        "\n",
        "    # y = x.flatten()\n",
        "    # plt.subplot(2,3,1)\n",
        "    # plt.hist(y)\n",
        "    # plt.title(\"conv1_partial_sum\")\n",
        "    x = relu(x)\n",
        "\n",
        "    x = pooling(N,6,14,14,x)\n",
        "    x = x * scales['conv1_output_scale']\n",
        "    x = np.clip(x.round(),-128,127)\n",
        "    x = convolution(N,16,10,10,5,5,6,x,conv2_weights)\n",
        "    x = np.clip(x,-2**16,2**16-1)\n",
        "\n",
        "    # y = x.reshape(-1)\n",
        "    # plt.subplot(2,3,2)\n",
        "    # plt.hist(y)\n",
        "    # plt.title(\"conv2_partial_sum\")\n",
        "    x = relu(x)\n",
        "    x = pooling(N,16,5,5,x)\n",
        "    x = x * scales['conv2_output_scale']\n",
        "    x = np.clip(x.round(),-128,127)\n",
        "\n",
        "    x = x.reshape(N,-1)\n",
        "\n",
        "    x = fully_connected(N,x,fc1_weights)\n",
        "    x = np.clip(x,-2**15,2**15-1)\n",
        "    # y = x.reshape(-1)\n",
        "    # plt.subplot(2,3,3)\n",
        "    # plt.hist(y)\n",
        "    # plt.title(\"fc1_partial_sum\")\n",
        "    x = relu(x)\n",
        "    x = x * scales['fc1_output_scale']\n",
        "    x = np.clip(x.round(),-128,127)  \n",
        "    x = fully_connected(N,x,fc2_weights)\n",
        "    x = np.clip(x,-2**15,2**15-1)\n",
        "    # y = x.reshape(-1)\n",
        "    # plt.subplot(2,3,4)\n",
        "    # plt.hist(y)\n",
        "    # plt.title(\"fc2_partial_sum\")\n",
        "    x = relu(x)\n",
        "    x = x * scales['fc2_output_scale']\n",
        "    x = np.clip(x.round(),-128,127)\n",
        "    x = fully_connected(N,x,fc3_weights)\n",
        "    x = np.clip(x,-2**16,2**16-1)\n",
        "    # y = x.reshape(-1)\n",
        "    # plt.subplot(2,3,5)\n",
        "    # plt.hist(y)\n",
        "    # plt.title(\"fc3_partial_sum\")\n",
        "    x = x + fc3_bias \n",
        "    x = x * scales['fc3_output_scale']\n",
        "    x = np.clip(x.round(),-128,127)  \n",
        "\n",
        "    # plt.show()\n",
        "    return x\n",
        "\n",
        "# def test(dataloader):\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     n_inferences = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for data in dataloader:\n",
        "#             images, labels = data\n",
        "\n",
        "#             images = images.numpy()\n",
        "#             labels = labels.numpy()\n",
        "            \n",
        "#             outputs = forward_C(images.shape[0],images)\n",
        "\n",
        "#             predicted = np.argmax(outputs, 1)\n",
        "#             total += labels.shape[0]\n",
        "\n",
        "#             correct += (predicted == labels).sum()\n",
        "\n",
        "#     return 100 * correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJXF18ZmXUv_"
      },
      "source": [
        "# score = test(testloader)\n",
        "# print('Accuracy of the network on the test images: {}%'.format(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import numpy\n",
        "import json\n",
        "#x=np.arange(12).reshape( (1,3,2,2) )\n",
        "#weight=np.arange(90).reshape( (5,18) )\n",
        "# x=np.ones(8)\n",
        "# x=x.reshape( (1,2,2,2) )\n",
        "# x=x+7;\n",
        "# print(x)\n",
        "input=np.arange(18).reshape( (1,2,3,3) )\n",
        "weight_conv=np.arange(16).reshape( (2,2,2,2) )\n",
        "#weight=np.ones(16)\n",
        "#weight=weight.reshape((2,8))\n",
        "weight=np.arange(16).reshape( (2,8) )\n",
        "# x = x.reshape(1,-1)\n",
        "#print(x.shape[1])\n",
        "def convolution(N,M,P,Q,R,S,C,input_data,weights):\n",
        "    output_data = np.zeros((N,M,P,Q))\n",
        "    for n in range(N): \n",
        "      for m in range(M):\n",
        "        for p in range(P):\n",
        "          for q in range(Q):\n",
        "            output_data[n][m][p][q] = 0\n",
        "            for r in range(R):\n",
        "              for s in range(S):\n",
        "                for c in range(C):\n",
        "                  h = p + r\n",
        "                  w = q + s\n",
        "                  output_data[n][m][p][q] += input_data[n][c][h][w] * weights[m][c][r][s]\n",
        "    return output_data\n",
        "def fully_connected(N,input_data,weights):\n",
        "    X = input_data.shape[1]\n",
        "    Y = weights.shape[0]\n",
        "    output_data = np.zeros((N,Y))\n",
        "    for n in range(N):\n",
        "      for y in range(Y):\n",
        "        for x in range(X):\n",
        "          output_data[n][y] += input_data[n][x] * weights[y][x] \n",
        "    return output_data\n",
        "x=convolution(1,2,2,2,2,2,2,input,weight_conv)\n",
        "x = x.reshape(1,-1)\n",
        "print(x)\n",
        "fully_connected(1,x,weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99JHuSuw77gM",
        "outputId": "c07acb59-7995-4366-baec-dff9dd846c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 268.  296.  352.  380.  684.  776.  960. 1052.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[21880., 60024.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def Prototxt_parse():\n",
        "  path = \"../example/app/build/test.csv\"\n",
        "  fp = open(path, \"r\")\n",
        "  data = fp.readlines()\n",
        "  fp.close()\n",
        "  for d in data[1:]:\n",
        "      row = d.split(\",\")\n",
        "      # ['N', 'C', 'H', 'W', 'M', 'R', 'S', 'U', 'pad', 'NUM_WORKER']\n",
        "      global N,C,H,W,M,R,S,U,pad,NUM_WORKER\n",
        "      N=int(row[0])\n",
        "      C=int(row[1])\n",
        "      H=int(row[2])\n",
        "      W=int(row[3])\n",
        "      M=int(row[4])\n",
        "      R=int(row[5])\n",
        "      S=int(row[6])\n",
        "      U=int(row[7])\n",
        "      pad=int(row[8])\n",
        "      NUM_WORKER=int(row[9])\n",
        "\n",
        "Prototxt_parse()\n",
        "\n",
        "\n",
        "E=(H-R+2*pad)/U+1\n",
        "F=(W-S+2*pad)/U+1\n",
        "n = (N*M*E*F) / NUM_WORKER\n",
        "mul_times=C*R*S\n",
        "\n",
        "for i in range(1,NUM_WORKER+1):\n",
        "  globals()['duplicated'+str(i)] = []\n",
        "\n",
        "for i in range(1,NUM_WORKER+1):\n",
        "  mul_counter=0\n",
        "  line_counter=0\n",
        "  outputfile= \"pe_\"+str(i)+\".trace\"\n",
        "  path=\"../example/app/build/traces/\"\n",
        "  f = open(path+outputfile, \"a+\")\n",
        "  for line in f:\n",
        "    line_counter=line_counter+1\n",
        "    if 'ST ' in line:\n",
        "      mul_counter = mul_counter+1\n",
        "    if mul_counter >= mul_times:\n",
        "      if 'BARWAIT' in line:\n",
        "        break\n",
        "      globals()['duplicated'+str(i)].append(line)\n",
        "  del(globals()['duplicated'+str(i)][0])\n",
        "  if i == NUM_WORKER:\n",
        "    n = n+(N*M*E*F - NUM_WORKER * n)\n",
        "  for output_times in range(1,n-1):\n",
        "    for word in globals()['duplicated'+str(i)]:\n",
        "      if '0x3' in word:\n",
        "\n",
        "        print(d)\n",
        "        print(type(d))\n",
        "  # for d in globals()['duplicated'+str(i)]:\n",
        "  #   f.write(d)\n",
        "\n",
        "\n",
        "    #print(line)\n",
        "    # if line == text:\n",
        "    #   mul_counter = mul_counter+1\n",
        "    #   if mul_counter==mul_times:\n",
        "    #     duplicated.append(line)\n",
        "  f.close()\n",
        "\n",
        "print(duplicated1)\n",
        "\n",
        "for i in NUM_WORKER:\n",
        "  # start_idx = i * n\n",
        "  # int end_idx = (i + 1) * n - 1\n",
        "\n",
        "  # if i == NUM_WORKER - 1:\n",
        "  #   end_idx = N*M*E*F - 1"
      ],
      "metadata": {
        "id": "VbFRPdioN2RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for i in range(1,NUM_WORKER+1):\n",
        "#   mul_counter=0\n",
        "#   line_counter=0\n",
        "#   outputfile= \"pe_\"+str(i)+\".trace\"\n",
        "#   path=\"../example/app/build/traces/\"\n",
        "#   f = open(path+outputfile, \"a+\")\n",
        "# globals()['duplicated'+str(i)]\n",
        "def Prototxt_parse():\n",
        "  path = \"../example/app/build/test.csv\"\n",
        "  fp = open(path, \"r\")\n",
        "  data = fp.readlines()\n",
        "  fp.close()\n",
        "  for d in data[1:]:\n",
        "      row = d.split(\",\")\n",
        "      # ['N', 'C', 'H', 'W', 'M', 'R', 'S', 'U', 'pad', 'NUM_WORKER']\n",
        "      global N,C,H,W,M,R,S,U,pad,NUM_WORKER\n",
        "      N=int(row[0])\n",
        "      C=int(row[1])\n",
        "      H=int(row[2])\n",
        "      W=int(row[3])\n",
        "      M=int(row[4])\n",
        "      R=int(row[5])\n",
        "      S=int(row[6])\n",
        "      U=int(row[7])\n",
        "      pad=int(row[8])\n",
        "      NUM_WORKER=int(row[9])\n",
        "\n",
        "Prototxt_parse()\n",
        "\n",
        "\n",
        "E=(H-R+2*pad)/U+1\n",
        "F=(W-S+2*pad)/U+1\n",
        "n = (N*M*E*F) / NUM_WORKER\n",
        "mul_times=C*R*S\n",
        "\n",
        "for i in range(1,NUM_WORKER+1):\n",
        "  outputfile= \"pe_\"+str(i)+\".trace\"\n",
        "  path=\"../example/app/build/traces/\"\n",
        "  globals()['df'+str(i)] =  pd.read_csv(path+outputfile)\n",
        "\n",
        "print(df1)\n"
      ],
      "metadata": {
        "id": "zvJTvq1zl7ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "NUM_WORKER=16\n",
        "\n",
        "ld_counter=0\n",
        "store_counter=0\n",
        "stall_counter=0\n",
        "in_loop = False\n",
        "f = open(\"../example/app/build/traces/pe_1.trace\", \"a+\")\n",
        "for line in f:\n",
        "  if 'STARTLOOP ' in line:\n",
        "    in_loop = True\n",
        "  if in_loop == 1 :\n",
        "    if 'LD ' in line:\n",
        "      ld_counter = ld_counter+1\n",
        "    if 'STALL ' in line:\n",
        "      stall_counter = stall_counter+1\n",
        "    if 'ST ' in line:\n",
        "      store_counter = store_counter+1\n",
        "\n",
        "print(\"ld_counter\",ld_counter)\n",
        "print(\"store_counter\",store_counter)\n",
        "print(\"stall_counter\",stall_counter)\n",
        "  # for d in globals()['duplicated'+str(i)]:\n",
        "  #   f.write(d)\n",
        "\n",
        "\n",
        "    #print(line)\n",
        "    # if line == text:\n",
        "    #   mul_counter = mul_counter+1\n",
        "    #   if mul_counter==mul_times:\n",
        "    #     duplicated.append(line)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "rLKMI3Zdag1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}